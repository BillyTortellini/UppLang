
- Switch:

switch address
    case .IPv4 -> ipv4

switch address
    .IPv4 -> v4
        something
        #fallthrough
    .IPv6 -> v6
        something
    default
        something else

TODO:
-----
 - Switch
 - Struct Initializer

Probably tomorrow:

DONE:
 - Downcast should probably check tag?
    Seems reasonable, how does the analyser store this so that the ir-generator knows it?
    Note: The downcast is currently the node.Statement thing...



How do I store the subtype hierarchy?

The questions the type mods presents are:
What do the type-mods need to fullfill:
 * Is equal would be nice
 * Is a subtype of b? (Has to look in the )


Ideas I would like to try out:
 * Parser checks if subtypes have the same name, and if so, changes it to some error-name...
 * When analysing a struct, we pre-determine the struct-hierarchy based on the node, so subtype-creation doesn't have to wait for type_size to finish
 * Type_Mods containing subtype-hierarchy (Only names of subtypes)
 * Base_Type of type points to structure
 * Subtype Type can contain general datatype (E.g. struct_template_instance or error_type (If hierarchy is false))
 * When checking member access searching the type takes a bit longer...


Questions: Should struct subtype members be allowed to have same names as base-type?
    Node :: struct
        index: int
        Expression: 
            index: bool
No...

How to deal with subtypes/members having the same names?
 * Parser cannot check it (As it's a multi line thing, also not parsers job)
 * Could check on workload-creation..., could be fine...

What if the subtype name isn't valid...


Thinking a little bit more about subtypes...
What if... and hear me out... if... fuck... bitches...

So arguably maybe type_mods and type_base aren't the worst idea overall...
E.g. type_base is still struct, but mods contain the given subtype
Also maybe it's possible to have subtype-info for normal structs, which just gives you the base info...,
and then you can check is_subtype_of()

Also we can analyse the subtype-structure of a struct without having to look at anything else currently

What if the type mods contain the structure of the subtype, e.g. 
the ID-Hierarchy

What about something like
Datatype_Struct*
    

So what are the type-mods used for:
They are used so that both the id of a type has to match (base-type) + the type_mods have to match...


Type_Mods :: struct
    pointer_level: int
    const_flags: int
    subtype_info: *Subtype_Info

Currently we have a lot of:

Datatype_Struct* structure
Datatype_Struct_Subtype* subtype;
if (type->base_type->type == Datatype_Type::STRUCTURE) {
}
else if (type->base_type->type == Datatype_Type::STRUCT_SUBTYPE) {
}

And the combination would be:
if (type->base_type->type == Datatype_Type::STRUCTURE) {
    do something
    type->mods.subtype
}

Where the subtype contains:
 sheet
But then we could have subtypes on non-struct types, e.g.
Subtype of Struct_Instance_Template, and this would also work fine...

And for member access it would be enough to have
Should array/Slice/Function have subtype upcasts?

// Definitly not by default, but I think casting could be supported here...
[]*Node.Expression -> []*Node

Subtype_Info:
    int hierarchy_level;
    Subtype_Info* subtype_of; // Only 0 if base...
    String* name;

switch Color
    case .RED
    case .GREEN
    case .BLUE

switch Color
    .RED
        What
    .GREEN
        The
    .BLUE   
        Fuck


switches are exhaustive without break
switches only work on enums and struct-subtypes

Do I want the case-keyword?
Don't think so



TODO:
-----
 * Subtype hierarchy
 * Switch
 * Fix testcases
 * Probably add testcases for a lot of shit...
 * Maybe do assignment where we just move the given type, e.g. no pointer/other for templates...

DONE:
 * Polymorphic subtypes (Struct instance templates) + matching
 * Check upcast to base type
 * Update Struct-Initializer
 * Struct initializer comptime calculation
 * IR-Code needs to set tag correctly




Polymorphic Subtypes and polymorphic Matching with constants should be thought about...
What about polymorphic sub-types (e.g. Subtypes of struct instance templates?)
foo :: (a: *Address(int).IPv4)






TODO:
 * Add testcases for constant, auto-address of and others





--------------------
- MISSING FEATURES -
--------------------
Core: (Required for programming)
    * Union rework, e.g. Addressable sub-types + better switch 
    * C-Function calling rework (Making it work again + being able to define the interfaces to c-functions/globals inside Upp)

    * Editor-Update (Remove token-based editor, motions + more vim commands, multiple-files, auto formating, goto definition, search-and-replace, ...)
    * Incremental compiler

Additional:
    * Allocator system
    * Better C-Importer
    * Code-Editor features
    * Code generation with #insert (Or something like that, maybe bytecode insertion? (But should somehow be checked for validity?))
    * Annotations (For automatization of print or serialization)
    * Put initial symbols (int, float, bool + hardcoded functions) into another namespace Upp/Compiler/custom thingy?
    * Auto function? .-> e.g. search(array, .-> x.name == "What")
    * Block-Expressions?
    * Conditional compilation (#if), probably only for multiple operating systems, maybe also for datastructures?
    * Macros?
    * Backtick defer, backtick variable definition
    * Varargs (Other syntax for pass as slice...)
    * Literal overhaul
    * Using on structs
    * Location information (Mostly for logging)
    * Debugger
    * Optional pointer types (e.g. ?*)
    * Const define infer

-----------------
Code improvements:
    * Deduplication of comptime instances when the generated ir-code is the same
    * Analysis-Pass hashtable could be improved by combining blocks of ast-code into regions (But!: Incremental parsing)
    * Symbol table lookups could be combined into a single hashtable?
    * Expression_Info could already store if value is comptime calculatable (Could be implemented similar to is_temporary_value)
    * Polymorphic Iterators should be hashed, so we don't have to instanciate/reanalyse header for every instance...
    * Remove polymorphic base-analysis (Could make lot's of things a lot easier, e.g. conditional compilation...)

-----------------
Missing Test-Cases:
    * Struct unpacking
    * Function overloading
    * Function polymorphism/implicit polymorphism
    * Imports

-----------------











Design Decisions to be made:
----------------------------
My original idea was that we would be able to specify multiple subtypes of some structures, e.g.

Game_Object:
    id: int
    transform: Transform
    render_subtype: variant
        Model ...
            mesh: *Mesh
            shader: *Shader
        Effect ...
        Foliage ...
        None ...
    logic_subtype: variant
        Player
            gamer_tag: String
        Enemy
        None

and then you could do something like
    player: Game_Object{Model, Player}
    player.mesh
    player.shader
    player.gamer_tag

    object: Game_Object


Advantages/Disadvantages:
-------------------------
 * Complexity is higher
 * Otherwise bettah
This is the definitely the more advanced system

Question: How would the compiler handle self-references to subtypes in this system?
--> You could cache the tags in the type-constructor, so you would never duplicate
--> If the subtype isn't finished yet we can add a waiting-workload to the struct
--> Once it finishes, the type can check if it's a valid sub-type or not...
  --> On member access we can check if it's an error, or if we should wait

Render_Object :: variant
    Model:
        mesh: *Mesh
        shader: *Shader
    Copy:
        template: *Game_Object.(Model, Player)
        mods: Mods

Logic :: struct
    Player
        gamer_tag: String
        input: *Input
    Enemy
        state_machine: *State_Machine
        state: int
        aggro: *Game_Object.(Player)

Game_Object :: struct
    id: int
    render_object: Render_Object
    logic: Logic
    
Further design things you could/could not do:
 * types of subtypes
 * types of pointers...


Ideas for variant syntax:
1. new keyword, subtypes have uppercase names
    Address :: variant
        IPv4
            bytes: [4] byte
        IPv6
            bytes: [8]byte
        Location
            street_name: String
            city_code: int
            house_number: int

2. Just use struct again
    Address :: struct
        index: int
        IPv4:
            bytes: [4] byte
        IPv6:
            bytes: [8]byte
        Location:
            street_name: String
            city_code: int
            house_number: int
What would be the problems with this approach?


Syntax for accessing subtype-types + values
    // For base-types, we can access members normally (+ tag exists and can be read/written to)
    a: Address
    a.tag = .IPv4
    a.IPv4.bytes = .[127, 0, 0, 1]

    // For subtypes, the subtype members are accessible
    v4: Address.IPv4
    v4.index = 15 
    v4.bytes = .[172, 0, 0, 1]

Syntax for multiple subtypes:
    // Either we go for some sort of stacking, like
    player: Game_Object.Player.Model

    // Or we use some new syntax for this?
    player: Game_Object.(Player, Model)


Implementation:
1. Remove union, c_union is now union
2. Change struct syntax, so that AST::Struct_Member can be a struct-subtype
3. Add parsing for this

I think implementing single inheritance is good enough for the start

    
Node :: struct
    Statement:
        Assignment:
            left: *Node.Expression
            right: *Node.Expression
        Delete:
            value: *Node.Expression
    Expression:
        Path_Lookup:
            fuck

lookup: Node.Expression.Path_Lookup
node: Node
if node.tag == .Statement
    stat :=* cast{Node.Statement} node

Variants:
---------

Vehicle :: variant
    Car:
        fuel: int
    Bike:
        electric: bool
        

Expression :: struct
{
    base: struct
        token_range: ...
    Binop:
        left: *Expression
        right: *Expression
    Unop:
        value: *Expression
        unop: enum
            NEGATE
            NOT
            POINTER
            DEREFERENCE
}

binop: Expression.Binop
binop.base.token_range
binop.left
binop.right

C++
Expression :: struct {
    base: Node
    union {
        struct {
            Expression* left;
            Expression* right;
        } binop;
    } options;
}


 * Size and member analysis similar to structs



Anonymous structs inside structs:
---------------------------------
    A :: struct(T: Type_Handle)
        b: struct
            c: T

Should this work?
    Probably, I usually use quite a lot of structs 'inside' structs

How does this currently work?
    When encountering anonymous structs, a new struct workload is created
    Same with #bake and others...
    Why exactly does this happen? Not quite sure anymore
    Then, the current workload waits on the new workload to finish, and
    accesses the results of the new workload.
To be fair, as we don't have more complicated struct workloads,
    we _could_ just analyse this like a normal workload

Currently:
    Anonymous functions/structs/bake create a new workload, and wait on that
    workload to finish.
Other solutions:
    As anonymous

General problems with internal definitions:
    * Internal/External symbol tables
    * Should the anonymous definitions be analysed multiple times?
        - In theory it depends if they are using polymorphic symbols...
        - I don't want to run too many bakes if I can avoid it...

Why is bake even a workload to begin with?
    For deduplication it kinda makes sense...

What are the approaches:
    1. Make it so that anonymous stru
    


-----------------------------------------------------


Can structs contain template types?
 No? I guess not, as all structs are unique this isn't an issue
So we can then answer the questions about the deduplication with a no.

When do I need constant values that are frigging graphs or some shit like that?
 - Probably never I guess
 - Strings? I guess this is kinda necessary if I want to do file loading or something like that
 - How hard is the de-duplication for graph-like structures? Not that hard... 
    1. Remove pointers from hashing
    2. Check if normal structure equals that of any existing value, if yes, recurse to check if the pointers are the same
    3. If everything is the same don't add shit, otherwise add shit...
 Btw. Not deduplicating upp_constants may also be a reasonable solution for now...

TODO:
----------------------------
Polymorphism related things:
 * Comptime function pointers should work
 * Better implicit matching -> E.g. error messages + better checks
    Store how many implicit parameters are per 'normal' parameter, and check if all match or if some are "lost"
 * Allow polymorphic return type (E.g. match with expected type in context, also needs dependencies)
 * Check how we should handle anonymous structs inside polymorphic structs/functions
Others
 * BUG-FIX: x:bool = 12 --> 2 error messages
 * Code improvements: 
    - Rename polymorphic type to something more correct
    - Maybe seperate a Overload_Candidate and Callable, and remove polymorphic parameters from Type_Function

DONE:
 * Upp_Constants should be deduplicated by default, I feel like this is an easy problem...
 * Handle default values for polymorphic arguments in some way (Probably just prohibit), what if default values depend on comptime values
    


Allocator :: struct
    allocator_data: Any
    allocate: (size: int, alignment: int) -> *void 
    free: (ptr: *void, size: int) -> *void 
    realloc: (ptr: *void, previous_size: int, new_size: int) -> *void

Hashable :: struct(T: Type)
    equals: (a: *T, b: *T) -> bool
    hash: (a: *T) -> u64

hashtable_insert :: (table: *Hashtable($H), element: *H.T) -> bool
    hash := H.hash(entry)
    import Compiler~builtin~modulo as %
    entry := *table.entries[hash % table.size]
    if entry.empty
        entry.value = element
        entry.empty = false
        entry.hash = hash
        return true





-----------
--- OLD ---
-----------

Things that need to work:
    - Base analysis
    - Instanciation

Implementation Order:
    - Add polymorphic-struct as expression-result + maybe as symbol?
    - Add tag to struct to mark it as polymorphic

Base-Analysis:
    1. Analyse arguments, create symbols
    2. During base-analysis, accessing symbols returns the correct type, but is not comptime known. (And shouldn't error on comptime known)
    3. Accesing type-types would result in an error type I presume

Instanciation:
    1. Functions calls work on polymorphic structs
    2. Analyse all arguments as comptime, if one fails return error type
    3. Otherwise create a new workload that also analyses the struct, but this time it uses the instanciated values on symbol lookup
    4. Implement symbol analysis of polymorphic struct by either being in the base analysis or not...

Error-Handling:
    Well if structs have errors, we can still generate and run code with the error types, so I guess this is how struct errors propagate



Polymorphism for structs:
    A :: struct(T: Type_Handle, C: int)
        x: [C]T

    add :: (x: A($T, $C), b: A(T, C)) -> A(T, C)
        result: A(T, C)
        for i = 0; i < C; i++
            result[i] = x[i] + b[i]
        return result

    // But also implicitly
    add :: (x: A) -> int

    Dynamic_Array :: struct(T: Type_Handle)
        buffer: []T
        size: int

So what do I need:
    Structs need to work similarly to functions: 
        1. First we need to have a parameter analysis
        2. We need to have both a base analysis with unknowns + instance analysis
        3. Instanciation requires argument/parameter matching, and re-analysing the struct...
            * On normal symbol read, throw error, only instances are valid...

How do structs currently work:
    1. Discovery: Difference between unnamed and named structs
        Both generate a new workload

Further Requirements:
    * Only named structs can be polymorphic
    * Structs cannot have implicit polymorphis T :: struct(a: $T)
    * All arguments are automatically expected to be comptime

Probably what's gonna happen:
    1. I have to rewrite struct dependencies, and remove the circular dependency thing
    2. Starting of with just named parameters which can only be types
        A :: struct(T)
            x: *T
    3. Then adding the rest gradually...


A :: struct (T)
    value: int

B :: struct
    x: A(B)

B analysis will create an instance of A(B), and if I add a struct dependency on this...
    During instanciation, I will have to analyse the parameters, and there I need member-in-memory?

This creates an instance of A(B), which then depends on the fucking size, although it doesn't work...

So new struct-analysis workloads:
    1. In module discovery check if struct is polymorphic --> Tag struct as polymorphic
        Create workloads
    2. Workload Struct_Analysis --> Analysis members and types of members 

Related:
 * Encountering struct symbol: Return the "unfinished" type
 * Member-access: Wait for member-analysis workload
 * Size_of, bakes or type_info that requires size: --> Wait for size_finish workload 
    So calling type_info in a bake/function may have to add all-reachable types as dependencies
    But calling type_info can be done on any arbitrary type_handle, not just comptime known types...
    can I access global type_informations array? Maybe just disallow that, and we'll be gucci,
 * Function compilation --> Wait for size_finish workload
    Here the problem is how to find out if functions require the size of structs 
    --> always on intermediate results (Variable register size), member access(offset calculation)
        Can I make ir_code wait on workloads? Probably not, but it would make things a little easier

So things I don't like with current code:
    - Different dependency types (Member in memory, normal, reference...) --> Seems hacky, when type-analysis afterwards would suffice
    - Array size analysis sucks ass
    - Functions cannot be analysed even when members aren't used
    - Not sure if system works with Templates



APPROACH 1: REMOVAL OF CLUSTER WORKLOADS:
-----------------------------------------
    --> Bytecode_Interpreter calls to type_info can add dependency to execution
    --> Bytecode_Interpreter calls to other functions can add dependency to execution (Other function compilation...)
    --> IR_Code generation can also add dependency on size_finished
    --> Removal of workload clustering (Struct finish and cluster compilation)

Problems with this:
    - Quite big change in architecture, e.g. lot of work --> means lots of bugs and other things
        To be fair, I don't even think that the changes are that big, ir-code just needs a current whatever and we are gucci
    - Bytecode execution now will have a stronger separation to compiled execution 
        But I think we already had that because compiled execution has more problems:
            * Turing completeness, cannot stop except if we run as another process
            * What if it calls file_systems and things and we want to do incremental compilation
            * Generally calls to extern functions...
            * Cannot track calls to type_info --> This one may actually be easily solvable...
    - Type_Information Array needs to contain pointers, because bytecode execution may halt
        and invalidate the array addresses (resize), access to the array should be forbidden
    - Bytecode Interpreter should have read only access to type_information array

Pros:
    + Less restrictions on workloads, functions can be compiled and run even with unfinished structs (Pointers/Slices)
    + No more cluster dependencies --> Less complexity



            analysis_workload_add_struct_dependency(current->progress, other_progress, current->dependency_type, failure_info);

APPROACH 2: NEW TYPE_SIZE WORKLOAD
----------------------------------
Structs and arrays store if the size is ready, otherwise pointer to size-ready workload
Alter Workload: Struct_Analysis, which after member types are analysed
    1. Calculates size and waits on other size workloads
        If cyclic dependency is detected, set the whole member to error-type
    2. Adds struct-reachable workloads by looking at member types
Symbol_Read of Struct: Add Cluster dependency to compile workload, but DONT wait on compile workload
Member-Access: Wait for size workload (Because member could change to error type!)
Array_Creation: If element_type still has to wait (Either other array or struct)
    Array-Type contains: Waiting_on_workload (Waits on struct size), 
    Struct-Size-Workload: List of arrays waiting on this struct (Finish in order! because of arrays of arrays)
    No additional dependencies needed, because struct dependency would have already been added



Some current thoughts:
    - Function_Cluster_Compile is probably usefull to not have to run functions before all errors are found
    - Bytecode_Interpreter calling type_info pauses is probably not even that bad, because
        maybe in the future we want also other functions calling back into the analyser...

Ok, so what I'm pretty clear about is:
    - Remove Struct_Reachable_Resolve, it's very hard to think about why this is needed and
        also why it even works (Not even sure it works in all cases). E.g. is it assured that all type_infos
        are initialized when the reachable resolve finished? Are all ways in which local variables are initialized
        assured to have the struct finished (E.g. bake returns type, macros other things in the future)
      Replacement:
        IR_Code Generator can now add dependencies on struct-size workloads (If type of member variable size isn't finished)
        Interpreter now also has to be able to add dependencies when calling 'type_info' 
    - Function_Cluster_Compile should probably stay, but I'm not sure yet why or if there are other ways to make this work
        if we could remove it, then the workload clustering could be removed, which is 1. probably slow and 2. hard to understand
        Also, we may want to move the function calls/gets called from modtree-function to ir-function

Implementation:
    Things that Bytecode should do:
        * Function-Code in separate areas of memory (So I don't have to keep updating references), each function tracks it's own references
        * Globals should be referenced by index, not by a memory address (Actually probably would cause problems with pointerz)
    Bytecode partial compilation kinda sucks, except that well references can just work if you do cluster compiling
        e.g. you create all stubs first and then you don't need to track the references
    Remove struct reachable resolve, only have analysis now --> 
        IR_Code generation now reports:
            * Structs required for finishing code
            * Function calls/references other functions
        After analysis of the function we just do compilation and then finish



CURRENTLY WORKING ON: Pausable Bytecode-Interpreter:
    FINISH TESTCASES
    Required changes:
    - Type_Info for c-interpreter must work again (Add internal type infos as constants and add array access)
    - Data accessed in interpreter must not relocate during pauses, this includes:
        * Internal_Type_Infos are currently allocated in dynamic array in type system, must be pointerz (DONE)
        * Constant pool access (DONE)
        * Global variable storage (DONE)
    - Write fucking testcase for constants with wacky memory stuff
    - Type_Info should check type size and pause execution






Design Things:
    - Implicit Operators, operator overloading
    - Structs/Union rework

Struct/Union rework:
Problem:
    If you do subtyping, where all types have different parameters, you cannot allocated them in an array...
    But this depends strongly on the current situation, so maybe I want to support both opertions?
    Also there is the problem with having things in read only memory or not, and other stuff...

Let's go through the approaches of different languages:
    Zig:
        * Tagged unions
    Rust:
        * Enums 
        * Dyn traits
    OO-Languages:
        * Usually only Inheritance
Final thoughts:
I think just having tagged unions for this is fine, but I would also want some mechanism to differentiate based on types!

Address :: union
    ipv4: [4]byte
    ipv6: String
    other: struct
        registered: false
        id: int

Operations on Tagged-Unions:
    a := Address.ipv4{.[127, 0, 0, 1]}
    * Get Union tag (Returns anonymous enum type) --> Maybe make builtin-function? Should also be pointer-able
        x := get_tag(a)
    * Set Union tag
        x: Address
        set_tag(x, .ipv4)
    * Get Union tag type
        x: union_tag_type(Address)
    * Switch over union
        switch a
            .ipv4 -> x 
                print_array(x)
            .ipv6 -> ipv6
                print_string(ipv6)
    * Accessing values:
        if get_tag(a) == .ipv4:
            a.ipv4 = .[1, 2, 3, 4]
        else 
            a.ipv6 = "Hello there!"
            

Problems and confusion with tagged-union "subtypes":
    * Difference between tag type and subtype:
        Address.ipv4   // This is a type
        Address(union_tag_type(Address).ipv4)
        a := Address(.ipv4)
        get_tag(a) == .ipv4
        a.ipv4     // This is a value, but not a subtype value...
        I think this just disallows the use of a.ipv6, which will result in an error on this type...
        // Can I use a as whatever now? --> No, I don't think so

    * Automatic casting  
        a: Address.ipv4 
        b: Address.ipv6 
        foo :: (x: Address) -> ...
        foo(a)
        foo(b)

Note: The above would be fine, the problem I am currently having is how to handle
      something like inheriting attributes

C:  
    struct Node_Base{
        Node_Type type;
        int index;
        const char* name
    }

    struct Node_Expr {
        Node_Base base;
        int literal_value;
    }
    
    struct Node_Definition {
        Node_Base base;
        String* id;
        initial_value: Node_Expr*;
    }

Translated to Upp:
    Node :: struct
        index: string
        name: String
        subtype: union
            expression: struct
                left: *Node(.expression)
                right: *Node(.expression)
                binop: enum
                    ADDITION,
                    SUBTRACTION,
                    MULTIPLICATION
            int_literal: int
            definition: struct
                id: String
                initial_value: Node(.expression)

    a: Node
    a.id   !Error: Node does not have a member id
    b: Node(.int_literal) = .{}
    b.expression =
    c: Node(.definition)

Problems that I see coming:
    What if specific type contains names that clash with upper namespace?
        A :: struct
            a: int
            b: union
                a: bool
                c: String
        x: A
        set_tag(x.b, .a)
        x.b.a = false

        y: A(.a)
        z := y.a !Which one?

    What if this is stacked?
        A :: struct
            b: struct
                c: union
                    x: int
                    y: bool
        
        x: A(c = .x) // Should this work?
        x.b // type-of = anonymous_b(.x)

        I feel like this shouldn't work

So for a union the semantics of subtypes are easy, because you can just treat the union as the type of the variant
    Address :: union
        id: int
        ipv4: [4]byte
        ipv6: String

    // Note: I don't think these should work, because we also need to initialize the tag somewhere?
    y: Address(.ipv6) = "Hello there"
    x: Address(.ipv4) = byte.[127, 0, 0, 1]
    z: Address(.id) = 15

    // Maybe the initializiation would require a struct-initializer?
    y: Address(.ipv6) = .{"Hello there"}

    a: int = z // Should work through a cast?
    b: Address = z // Should also work (upcast)
   
But this already seems weird, as you can use the union as another type...
    

Why do I want to be able to reference subtypes in the this way in general?
    1. Type-Safety: 
            Makes the program more save, and also abe to model more specific information:
             * I cannot just assign any Union to the specific variant, I first have to do a check (Maybe putting it on auto-cast)
             * Accessing a non-active member throws an error 
    2. Convenient member accesses:
        By knowing the specific variant, there is no need to specify the variant for member accesses anymore
            x: Node
            x.subtype.expression.left
            y: Node(.expression)
            y.left

            
        





Notes:
    Why don't I do comptime calulcations of all expressions? Not the calculation, but the fact if it is comptime or not?
        Note that we could calculate for some functions if they include some sorts of recursion, and if they don't, we are fiiine



TODO - Implicit Parameters:

DONE
    * There currently is no differentiation between $T and T in matching, but only $T should be matched
        --> This means that we should have 2 polymorphic types (Which are connected, one a reference and the other normal...)
            --> Symbol-Lookup return the reference type, whereas $T returns the real thingy
    * get_info in analyser_expression_internal causes errors because we analyse the parameter type multiple times currently
        --> Do the filling out part with types, move the whole thing to type-system file
        --> Set expression info and already analysed --> No need to analyse twice!
        --> Use another (lol) analysis pass again --> (Not gonna lie, this seems kinda ok for this use case)
    * Polymorphic Type should be treated like error type (E.g. don't log errors, just do not runnable)
        Check all things where we handle "error type", and also add check for polymorphic type...
        Situations:
            - Unary/Binary operators
            - Casting
            - Auto enum, struct initializer

    o. Make two different types for Polymorphic Type: Base-Type and reference. 
        This is required because for matching only the base-type should be matched, and references shouldn't count...
            e.g. foo :: (x: struct { a: T, b:$K }, a: $T)
            Here when analysing x, the value for T shouldn't be set!
    

        


All things that are functions:
    * Normal functions
    * Anonymous functions
    * Bake functions
    * Polymorphic functions
    * External Functions
    * Function-Pointer
    * Hardcoded functions




Main usage for templates:
 - Generic data-structures (E.g. dynamic-array, hashtable)
 - Math library (vectors, matrices, ...)

This means that symbols can now also be comptime functions

Hashable :: struct
   type: Type
   hash: (T: Type) -> u64
   equals: (T: Type) -> bool

Hashtable :: struct(h: Hashable, Value_Type: Type)
    entries: []struct
        valid: bool
        key: hashable.type
        value: Value_Type
        hash: u64
    element_count: u32
    
    


Design-Goals/Requirements:
 * Editor Requirements:
    - Line-based editing
    - Auto-Formating
    - Code-Folding, Block-Focus, Navigation, Advanced-Information display
 * Undo-Redo (History)
 * Incremental Parsing
     - Indexing from AST to Source-Code (With ranges)
     - Difference generation from history
 * Huge code-files 

Simple array of lines + indentation per line:
 + History commands easy (Add/remove indentation, change line text)
 + Editing is easy
 - Scales badly with huge code files
 - Line-Indices are absolute, every single index changes after a certain one
 - Blocks need to be identifyable for incremental parsing

Hierarchy:
 - History is hard (Merging/splitting blcoks)
 - Editing is hard
 + Scales good with huge code files

If I do precise inter-block changes, what needs to happen for each change?

First off, how will to algorithm to re-evaluate Items look like:
  Go through all lines/items simultaniously
   * If a line was added, parse the line, check if its an item, check if the next item needs reparsing
   * If an item is marked as removed, remove the item from the list?
   * If a item was changed

What about lines that aren't items yet? e.g. Empty-lines, lines with errors?
There is no error statement, so how do they work? Just save it as an added line?
Removed lines don't matter if there are no items





Incremental Compilation General Thoughts:
-----------------------------------------
To have the fastest possible compile-times, I want to have incremental-compilation,
which means that I only want to recompile the parts of the code that have changed since
the last compilation, and reuse as much Information from previous compiles as possible. 

Since the Compiler works in different Stages, each stage needs to support incremental compilation.
All Stages:
 - Input (From editor or from file)
    Output = Source-Code split into blocks and lines(Text)
 - Tokenization
    Output = Tokens in Lines in Blocks
 - Parsing
    Output = AST for each input 
 - Analysis
    Output = Annotated AST (Analysis Passes) + Modtree_Program
 - Code-Generation 
    Output = IR_Code + Bytecode + C-Code

For each stage we need to make a Decision:
How Granular should the change detection be?
    The changed objects could be Files, Blocks, Lines
    We could have 'simple' detection like: Item removed, item added, item changed,
    or we could have more sophisticated detection like: Item moved, item renamed, item merged...

This means logical Decisions need to be made on the Granularity of the Change-Detection based on the 
different Compiler-Stages.

There is an obvious Trade-of between Granularity, Efficient-Change detection and efficient compilation.
The more granular the Change-Detection, the harded/slower it is to find all Changes, but 
more granular Detection leads to better performance in compilation, since fewer Items need to be compiled.

Another Question is how to detect the changes. For this to work we need to generate
a change-log to the previous Version, and update our current version accordingly. 
One solution is to generate the new version independent of the previous version (E.g. parse the whole file again),
and then generate the Change-Log based on a diff between the two.
But it will be a lot faster to build diffs from previous diffs, e.g. we build the AST-Diff from the Source_Code-Diff
we have from the Editor, but there may be situations where we don't have a change log available (E.g. loading a different file, file changed)
In these cases re-analysing the item and doing a diff will probably be the best solution.
It may also be usefull to combine Diff-ing, Change-Log and the previous Version so that we can filter out Changes that didn't have any effect,
like adding a comment inside a function, which requires the AST to be reparsed, but the resulting AST used by the Analyser will be the same.

Another problem is that of Dependencies and references. Tokenization and AST-Parsing aren't really affected by this,
but Analysis and Code-Generation are. For example, adding a struct member requires all function that use this struct to be
recompiled, same with function calls when parameters change. For this we will probably need
to change the Dependency_Analyser to keep all references, and update accordingly, which will most likely be the hardest part when
implementing incremental compilation.

Yet another problem is that of Handles between stages. This includes:
 - Token-Ranges connect the AST to the Source-Code
 - AST-Node Pointer and Analysis-Passes connect the Semantic_Analysis to the AST
 - IR functions/globals have pointer to modtree-functions/pointers

Now lets take a look at which Granularity we'll be using at each stage, and how I plan to detect Changes
 - Input: Source-Code
     Here we get the Difference from the Editor-History, so we have exact information on how the text changed. (Single character inserts/removes)
     If we have project-imports (E.g. file loads), we need to check if the file has changed (File-Listener or check last-access time) and
     if it changed we could either do a diff to the previous version (to have incremental parsing), or we just reanalyze the whole file,
     which is probably the best solution for now since loaded files rarely change during editing (Except when we are editing 2 projects at once),
     but this isn't something the editor can handle currently.
 - Tokenization:
     Since our Syntax is designed that each line can be tokenized independently, and we cannot simply re-tokenize single tokens
     because the line-context is importent (E.g. adding a " at the start makes the whole line a string-token), 
     it makes sense to actually have a looser Granularity and compress the exact Text-Changes to line inserts/line-removes + line-changes
 - Parsing:
     The parser keeps track of bounding ranges for each ast-node, which would enable us to reparse individual ast-nodes, but
     parsing is also context-sensitive, and differs from tokenization because AST-Nodes regularly span multiple lines/blocks (E.g. Modules, code-blocks, structs...)
     This seems especially annoying when we edit the header of a block, since parsing all child items depends heavily on the Context the header provides.
     I currently believe that caching should be done on a block level, where we store the previous result + the Context the block was parsed in
     The currently valid block contexts are: 
        1. Module blocks, containing
            Imports
            Definitions
        2. Code-Block, containing
            Statements
        3. Enum-Block, containing
            Enums
        4. Struct-Block, containing
            Definitions
        5. Switch-Block, containing
            cases
     After incremental parsing we also need to know which ast-items were added/removed/changed. Do we need changed, or can we represent it by removed/added?
 - Analysis:
   

Think about: Different Compile-Types:
 - Parse only (Maybe for editing, e.g. formating code based on Parse-Info)
 - Analyse only (Used by editor for Syntax-Highlighting and Error reporting)
 - Analyse and run Bakes (Since we don't want to run bakes during editing)
 - Generate_Code (For running the program)

When to execute Compile-Time-Code-Execution (#bake, maybe #run in the future)?


Better Editor Experience:
 - Auto complete with partially typed == Keywords destroys the keyword
 - Code-Rendering Rework/Formating Rework (Only format whats visible)
 - Smarter Code-Formating (Splitting long lines + merging short blocks)
    Probalby Requires a rework of Formating in general
 - Camera commands (move cursor + move camera)



Next features: (OLD)
--------------
 * Syntax-Guided Editor (Undo-Redo, copy-paste, code-completion, jump to definition, auto-formating, vim-commands, animations, code folding)
 * Polymorphism/Generics/Templates (Template rework: No more templated modules, templated types, templated functions)

 * Named function parameters/unnamed struct initializer params
 * Bake function pointers
 * Type System switch to global
 * Always analyse struct initializer/function parameters, even if the function type is not known (Turning off error reporting during this)
 * Binary Operations should support auto-operations (enum, array, struct-init)
 * Loading files introduces new symbols, which could conflict with the old ones...
 * Rethink auto pointers with pointers on the left side of assignment, maybe just use pointer assign syntax &=
 * Switch with indentation, remove case keyword
 * Switch case variables 
 * Error messages using the AST::Display
 * Auto block-ids (if cond_var) should not work if there are 2 instances where we use the same cond, then just disable auto ids (Parser thing)
 * For loop and looping over arrays/slices pls
 * Slices as varargs, e.g. printf(a: string, values: []Any ...)
 * Conversion function to pointer and back
 * Literal overhaul (Integers, floats, strings?, arrays are u64, array access with any int...)
 * Array bounds check
 * Expression-Blocks

 * Macros and Iteration 
 * Generating source code with runtime code execution (#insert_code {return "what the fuck";})
 * Annotations (Used in generating source code)
 * Static Analyser

 * New with values?
 * Interpreter upgrade, call C-Functions, Custom allocator, check memory access
 * Function overhaul, e.g. Non-Pointer parameters constant, Named parameter values in function calls
 * Read-only variables
 * Struct unpacking syntax, ignore syntax 
     index, found := find_elem(arr, 2);
     index, _ = find_elem(arr, 3);
     _ = Hashtable~insert();
     - Analysis can be out of order in e.g. Parameter analysis, which may make things like Polymorphic Types easier.
     union {ipv4: int; ipv5: _};


Required Features before I can use the Language:
 * Syntax-Editor features 
 * Incremental Compilation
 * Templates
 * Context-System
 * Symbol-Imports/Resolution
 * Iterators/Operator overloads (Maybe in some sort of context system?)



------------------------------
 THOUGHTS ON RENDERING/EDITOR
------------------------------
Rendering only whats visible:
    Rendering + Formating are 2 different things
    Formating currently is just figuring out token displacements (spaces between tokens, space before/after cursor) and line-indices

Current Rendering Features
    - Rendering Tokens with Formating
    - Underlining pieces
    - Changing background color of pieces
    - Setting color of Tokens
    - Displaying Block-Indentation lines
    - Showing context info (Text box with text)

Thoughts:
---------
The formating/layout is importent for cursor movement, so it is required to synchronize the formating
before some operations (Mainly movement operations)
So it seems like formating should also be done by looking at the changes and reformating the lines that have changed.

And there is the Questions what the Output of formating is/how it is saved. 
Because I can still do the 'final' output in rendering


 -------------------------------
Code Completion Design Analysis:
--------------------------------
Enter Code-Completion Mode when pressing the Code-Completion button (E.g. CTRL+Space)
When we have multiple options to select pressing one key advances all

Questions: When to enter/exit code-completion?
    Maybe just have a toggle (CTRL+Space) for code completion, and don't exit until that toggle is off

Maybe we can always have code-completion except when we type definitions?
    Maybe we can enter/exit this mode when in insert-mode with shortcuts
And if we change definitions we can also change all occurances?
    Thats probably a bit too far
Also I want a 'Recursive' add-indent (Ctrl+Tab)/remove-indent(Shift+Ctrl+Tab) and I 
want my multi line comments back (E.g. Lexer has a tokenize all and tokenize line checks all parent blocks for comment start)


RANT:
-----
After adding Code-Suggestions (Editor views a list of symbols that start the same as you have already typed)
there was a decision to make on how to 'Select' one of the Suggestions and insert it into code
 1. Having a 'Insert Suggestion' Key
 2. Custom Completion Mode where everything gets inserted as you type

Completion Mode Afterthoughts:
At first I thought that completion mode may be insane because you could just always
be in completion mode, and you would never have to type identifiers out again.
But this causes problems:
 - Definitions: To type new identifiers we cannot have code-completion
 - Keywords: I could add keywords to code completion, but most of the time you just want to write them out
Also there is the problem that Completion-Mode does not allow for 'fuzzy' search, meaning that
it doesn't allow 'browsing' of code.

New Design Ideas:
Have a simple 'fuzzy' string search, that checks that all characters are typed correctly
Always display completion ideas, and insert the first search when we hit insert_completions

Completion Mode #2:
After writing an identifier, and if it is not a definition + not a keyword,
always apply my fuzzy code completion because it is nice



----------------------------------------------------------
--- RANT ABOUT COMPILER/EDITOR STRUCTURE + PERFORMANCE ---
----------------------------------------------------------
 *** General Compiler/IDE Structure:
If I don't want a multi-file per project structure, I almost need something like incremental compilation. (Maybe)
Then theres the problem that I don't want to do Compile-Time Code-Execution during editing, meaning I want to store the results
of these bakes somehow over multiple frames/edits.
The problem here is that I want the editor to display/use Compile-Information (Errors, Warnings, Symbols, Code-Completion, Goto-Definition),
and the Compilation Process/Semantic Analysis may be a relative slow process, but the Editor needs to stay responsive.

 *** Current Thoughts on a Solution:
Lexing and Parsing is done incrementally (By the Editor) in 'real-time', meaning that the Editor never needs to wait for AST-Parsing to be done.
This is mostly because the Editor may need AST/Parse Information for Auto-Formating (Required for Rendering + Animations) and for Code-Views (If I implement them)
I think this is a valid Decision because I should be able to do Parsing incrementally with the current Syntax of the Language.

Maybe Semantic-Analysis is also one of the parts that can be done incrementally, but Workload-Dependency Management and Symbol Resolution will be the
problem. (Or I at least believe so)

Then the other two Parts of Compiling would be Semantic-Analysis and Code-Generation.
In the ideal case I would also execute those two incrementally, but I believe that waiting for Analysis before querying the Code
may be quite slow (Although Incremental Semantic-Analysis would probably be pretty fast). One of the slow parts may even be the
#Bake and #Run parts of the code, which I wouldn't even need to rerun if everything works out.

I generally believe that everything must run incrementally (Parsing + Analysis + Code Generation) eventually, since I want to be able
to generate huge Programs with this language and the IDE requires fast Feedback for editing.
The only thing that I am unsure about is if it is fast enough to do a complete Analysis after every edit. 
The Answer is probably yes.

And if the answer is no (No compiler feedback after every edit), I will have to draw a line somewhere (Probably after parsing)
where a second Thread does Analysis, and compiler feedback will have to be based on a cached, old Version of Analysis-Info.
But even with the caching and old version stuff I probably want some sort of incremental Compilation, otherwise Compiling and Running the
Program will become quite hard.



-----------------------------------------------------------
--- RANT ABOUT DATASTRUCTURE FOR CODE (Editor + Parser) ---
-----------------------------------------------------------
Requirements for Code-Representation
 - Lines as Text
 - Mapping from text to tokens
 - Inserting/Deleting/Moving characters, lines, blocks
 - Tokenization is line-independent (E.g. each line can be tokenized seperatly)
 - Navigation with a Text-Position
 - Parsing needs to store Start+End Position and requires Hierarchy-Infos
 - Text/Token Animations require previous positions (Rendered) of all Tokens (Maybe of Text?) + Changelog

Previously this was easier because: 
 - Text was just one Array of Lines
 - Tokens were just one Array and weren't structure line by line
 - Token-Infos could just also just be stored in one array
 - Lines could be accessed by index, and deleting lines was just array manipulation

Current Hierarchy structure:
 - The main Program is a block
 - Blocks contain multiple lines
 - Lines can have a follow block
 - Lines contain an array of tokens, lines must be tokenized from start to end

How about:
 - Text just beeing an array of lines, each line having an integer for indentation level

Source-Code Information required for Parsing:
 - Tokens + Line-Hierarchy + Comments

Information for Editing/Rendering:
Difference Between Edit/Rendering: Update/Render is in another order
 - Text
 - Cursor Position
 - Token Highlights
 - Token Positions/Colors

Decision for now:
 - Own hierarchy representation for AST-Parsing which gets generated every time from Text
    Source_Block(Array), Source_Line(per block)
 - Gives the Editor a way to have a different representation for text if necessary
 - This also allows filtering out multi-line comments before Parsing (Not normal comments because they can be at the end of a line)


Editor:
    * Has simple array of lines with indentation
    * For parsing a Hierarchical Representation is generated each time
    * Lexing is also different, now the editor has to manage spaces before/after the cursor


Problem: Multi-Line Comments
    Tokenization cannot be done line by line anymore
Solution:
    Just have single line Comments, and have an editor feature for multi line comments
    Also maybe have comment-handling logic to just add comments when inside a comment



Problem Analysis: Data structure for Source-Code (Tokens)
Infos:
------
Editor and Parser require different Data for efficient processing:
Editor:
    String of Text for editing (Per line)
    Render-Info for positioning, animation, color (Per token)
    Maybe Extra Block information, like indentation, is_collapsed, ...
    Looping over Token-Ranges and set color/draw underlines/display errors
    Comments need to be stored in representation

Parser:
    Lines containing Tokens
    Having follow-up information (Block follows line...)
    Generate Mapping AST --> Tokens (Token Ranges for AST-Items)
    Cheap Checkpoint rollbacks

The main difference is that the Editor needs to store Render-Information/Editor-Information
for some parts of the Source-Code, which is in conflict with the Requirements of the Parser

Another Problem is that while Editing it should behave like a normal Text Editor, with
each Line having its own indentation leading to edits changing the Hierarchical structure of the program

I also want to only do operations on the structure through Commands which can be saved in a History.
These Commands need to be Re- and Un-Doable, so that a traversable History can be generated.

Comments/Multi-Line comments are also a crux currently, since Parsing should ignore comments
alltogether, but I need to store them and treat them like normal tokens in the Editor



Solution 0: Current Solution, generate Hierarchical representation for Parser
-----------------------------------------------------------------------------

Solution 2: Linear Code (with indentation) for both Editor+Parser
-----------------------------------------------------------------

Solution 3: Linear Code (with indentation) + Hierarchy that is kept in sync by the editor
-----------------------------------------------------------------------------------------

Solution 1: Hierarchical Code, Editor-Infos annotated
-----------------------------------------------------
What I am currently thinking about is having one Hierarchical Representation for Tokens,
which contains Navigation Information (Blocks and Lines), and an easy way to annotate
this Tree with additional information.

Then the Editor would apply Hierarchical Changes to this tree (Add/Remove Block, Add/Remove lines, Add/Remove Indent)
and have the required Editor-Informations in the Annotations

Also maybe I want to have the Text and Tokens inside the Hierarchical Representation



Question:
    Shouldn't I just store the Render Info inside tokens?

Source Code Design:
Facts about the Data:
 - Lines holding Tokens Array
    * Maybe also Text-String
 - Blocks containing Lines + other Blocks (Hierarchy)

How do we represent the Hierarchy?
 - Implicit (Each line has seperate indentation)



Requirements:
 - Navigation (Token_Position + next_token, previous_token, next_line, previous_line)
 - Adding/Removing of Lines (Blocks should get destroyed if no lines exist anymore)
 - Indentation Handling (Adding/removing indentation)
 - Token-Layout (Position, size, color) for each token

How to design a Data-Structure:
 - Why is a Datastructure necessary/why can't items be processed one by one
 - Which operations needs to be executed on the Structure in which frequency (Also information querying)
 - Is it a built-once/read-only structure or is it changable
 - Which elements need to be addressable (Handles/Pointers)
 - Complexity-Performance tradeof

Hierarchy:
 + Parsing better navigation
 + Local Changes --> Better incremental Compilation
 + Performance scaling
 - Bad Editor Navigation
 - Bad Indexing (line index, block index?)
 - Indentation based Editing is complex
 - History Complicated
 - Complex Datastructure
 
Design Decision:
    The editor doesn't keep the tokens up to date, but it makes sure that lines are always sanitized after edits.
    When we need updated tokens we run through the change history from the last checkpoint (Last timestamp where tokens/text was synchronized)
    and only tokenize the lines that require tokenization

Why is a Code-Hierarchy usefull?
--------------------------------
I think it has to do with 'coordinates' of lines, since
Line-Inserts/Deletes change the line numbers of all lines coming after them, even if they are not changed.
This would mean that all Structure holding line numbers (Parser, AST-Nodes having start and end tokens) will invalidate
their references once the line numbers changes.
Also I think most logic concerning code is built upon the hierarchy, meaning that e.g. Iterating over the code (during parsing) would be easier in
the hierarchy.

 *** Next Day Thoughts:
 It could be that I want the Source-Code Representation to also be hierarchical, because it seems to make more sense 
 for incremental Parsing. This would also solve the problem of having 'slow' line-editing with only a single array.
 The real problem here is that indexing specific lines would be hard otherwise, because an insert into an block-child of a block will
 also invalidate all line indices that come afterwards. 
 
 *** Thoughts (27.06):
 So the Problem still is if we need a Hierarchical Representation of the Code, or if Lines + Indentation are enough.
 If we have both, we have to deal with synchronizing both datastructures, which is annoying. 

 The linear version is good because it's simpler, but the Hierarchical version has several benefits:
  * Better Editing Performance
  * Indexing is based on changes
  * We can store information per block, because it is a consistent structure
----------------------------------------------------------------------------
----------------------------------------------------------------------------






General Plan for Implementation:
--------------------------------
I currently feel like I should be programming something with the language I have created.
I am currently at the point where I would like to select the features needed before I can
create a simple application, which would be:
 - Templates (Datastructures)
 - Syntax-Based Editor
 - Module/Project System
 - Context for Allocators
 - Iterator Syntax, custom iterators for stuff
 - Literal overhaul, u64 for array size, automatic array bounds checks

Features that I DON'T need for the first simple game version:
 - Comptime Code generation
 - Annotations
 - Static Analyser
 - Macros



Error messages for circular dependencies:
 - When there is no progress, and no running workload can run, we have an error.
   This error could either be because we are waiting for a symbol, or because
   a circular dependency exists. 
   This means we then need to check for circular dependencies, and if one
   exists, we must report the error and resolve the circular dependency.



For unknown-spreading, we also need an Unknown Comptime-Value, where
we know the Type of the comptime-value, but it is currently unknown

Because if the Comptime-Value is unknown, but the type is right, we dont
need to log an error. But if the Type is wrong, we log an error

This also implies that I need an Array-Type with unknown size, because I don't want to
report an error on [unkown comptime int]byte

Hashtable :: struct (Key_Type: Type, Value_Type: Type, resize_percent: float)
    entries: []Entries;
    count: int;

create :: () -> Hashtable

double_array :: (Key_Type: *$T, Value_Type: T)
    
double_array :: (ip, i)




The Job of the Semantic Analyser:
 - Globals, Functions, Extern-Functions and entry function
 - Determine the Type of all expressions
 - Error Reportin

What are the most important/difficult saved things in modtree:
 - Expression result type + cast information
 - Modtree_Function (Called From/Calls, contains errors/is_runnable)
 - Modtree Parameter/modtree variable (For symbols to have a reference to)
 - Comptime results
 - Block control flow
 - Blocks (break/continue ids) are resolved
 - Extern Functions
 - Bakes are intern functions
 - Assert is also a function -> e.g. cannot be done with modtree...
 - Global init function

Ideas for AST-Decorations:
 - Symbol Reads are dependent on the Analysis-Pass 
 - Merge Analysis-Pass with analysis Progress

Relations between
 - Analysis Item
    Partition of AST into Items with Symbol-Dependencies
 - Analysis Workload
    Through Symbol Dependencies and internal Dependencies workloads depend on other workloads
 - Analysis Progress
    Progress of Functions, Definitions, Types to create dependencies between workloads
 - Analysis Pass
    One instance of an Analysis-Item with AST-Annotations



Why are AST-Decorations better than generating Code directly during Analysis:
 - Simpler analysis because I can split order of execution and order of analysis
 - Modtree currently has to duplicate AST-Structure, which isn't necessary with Decorations, modtree can be removed later
 - Analysis can be out of order in e.g. Parameter analysis, which may make things like Polymorphic Types easier.
 - Switching to stop and go will be easier
 - Allocation are easier, since I can do it with passes/arrays
 - Code-Generation is easier with Instruction/Register based code than with Modtree's Tree-Based code (Temporary values, return + defer...)

Problems:
 - AST-Nodes may need to be traversed multiple times (Macros, Templates), which is why I cannot
   just write the Analysis Informatio into the AST-Nodes
 - Code-Generation Step afterwards is more involved then current IR-Gen from Modtree
 - If I need to save Analysis-Information which is not easily saveable into the AST-Structure, it will be more
    complicated to store this data. 
 - Modtree removes all type-related things from AST (Enums, structs...), which the analyser still needs to annotate






DECISIONS:
----------
I want to have bake as it's own Workload, since the usefulness isn't too great currently.
Also I don't want to worry about definign Symbols in coming Analysis-Passes, since this is
also a Use-Case for a later Time.

Dependencies and Polymorphism:
    Currently you can have 2 Dependencies on a Function: Header-Analysed or Compiled (Bake)
    If we have a bake workload depending on a function, we would automatically add it to the
    Compiled workload. 

    How to Instanciate a Polymorphic-Function?
    First you create a new Body-Workload for the Function, where a mapping of all Polymorphic-Parameters
    to their Values is stored. This Body-Workload requires the Body of the Parent-Polymorphic Function to be finished.
    This ensures that all Symbol-Reads are done for the Body, and the Analysis can begin.
    To continue analysing the current Item the Instance-Workload isn't required to be finished.




IMPL:
-----
Parameter depends on Comptime:
    doubler :: (a: T, $T: Type) -> Type
    array_double :: (a: [Count], $Count: int) 
Comptime depends on Comptime:
    weird :: ($a: T, $T: Type) 
Circular dependency:
    fugg :: ($A: B, $B: A) 

Changes TODO:
Poly Analysis:
    - Somehow extract the dependencies between parameters 
        I only get this information during Symbol-Resolution in the Function_Header Workload
    - Analyse Comptime parameters first, afterwards normal parameters

This isn't that hard of a problem with implicit Polymorphism, since there the RC-Analyser
can alread set the Symbol to Type Implicit_Poly_Param (Or something like this)

But how do I store the Type of a parameter depending on Polymorphic-Arguments after Poly-Analysis/What to do during Instanciation
    doubler :: (a: T, $T: Type)
    doubler :: (a: *T, $T: Type)
Well I need to store it as a Polymorphic Type 

Idea:
    New Type_Type: Polymorphic Type


Polymorphic Procedures Implementation Plan:
-------------------------------------------
Lets start off with the simplest polymorphic function
    foo :: ($count: int)
    foo :: ($T: Type) {
        x: Type;
    }
    foo :: (a: [count]int, $count: int)
    foo :: (a: $T)
    foo :: (a: *T, b: $T)
    foo :: (a: table.Key_Type, table: Hashtable)
    foo :: (a: $T, b: #bake mallest_member_type(T))

I obviously cannot determine the final Type-Signature of the Polymorphic function without an Instance of it.
But, similarly to analysing the body without knowing the types, I should try to analyse as much of the
header as I can before analysing the body.

This is only required so I have more information about the parameter types. Otherwise I could just set the
parameter types to Unknown for all parameters during the body analysis

foo :: (a: [count]int, $count: int)
    1. Determine dependencies of the parameters
        a: Has a Dependency on the 2nd parameter of the function
        count: No dependencies on any other arguments
    2. Do DAG and find analysis order, save for later
        count, a
    3. Analyse parameters in the given order, if no order is found, log error and analyse anyway

This sucks again:
    foo :: (a: (x: $T, y: T) -> (T), value: int)
    foo :: (a: [$T]T) 



The problem with something like
    foo :: (x: *$T) {}
    i: int = 5;
    a := foo(&i);

Ok, since I have the 
    x: Hashtable(int) = initialize...
    foo :: (table: *Hashtable) {}
    bar :: (table: *$T) {}
    foo(x);     // In this context I can see that I require a pointer level of one above my own, so I work with that
    foo(xp);    // In this context, I see that I require a pointer level of 1, so I just keep what I have
    foo(xpp);   // Given: Pointer Depth 2, wanted: A singular pointer level, 
    bar(x);     // Requires pointer, so we do address-of, T = Hashtable
    bar(xp);    // Match *, match T = Hashtable
    bar(xpp);   // Match *, match T = *Hashtable

Since we have Auto-Dereference + Auto-Address_Of, do we have ambiguities?
    ipp: **int;
    foo :: (a: *$T);
    foo(ippp);  // T = **int
    foo(ipp);   // T = *int
    foo(ip);    // T = int
    foo(i);     // T = int

The questions I am having are:
    How to Analyse + Store Parameters during Header-Analysis?
    What to do when Instanciating a Polymorphic-Function?



Type-Constructors:
    - T                 ID
    - *T                Pointer
    - [5]T              Array
    - []T               Slice
    - (x: T)            Function-Signature
    - struct {a: T}     Struct
    - Enum {RED :: 5}   Enum
Type-Constructors form a tree

Features Simple to hard:
------------------------
Comptime Parameters:
    foo :: ($count: int) {return count * 2;}
        Problem: During Header-Analysis, count value is unknown, but type is known
    foo :: ($count: int) {return #bake(int) {return fib(5);}}
        Problem: Do I want to execute bakes during analysis? I do not think so.
    foo :: ($count: int) {a: [count]int}
        Array with correct type, but unknown size, do I need an extra type for that -> I think so
    foo :: ($T: Type) {a: T; return get_t_value(a);}
        Since I don't know the type of a, I also cannot determine if get_t_value works.
    bar :: ($T: Type, a: foo(T)) 
        Here foo should work with T, since T is unknown, but there should be an error when trying
        to use the return type of foo as a Type. Then the parameter A is set to Unknown

Parameter Referencing:
    foo :: (array: [Count]T, $T: Type, $count: int) {}
        A new Symbol-Table is needed where parameter names can reference other parameter names.
        It is reasonable to define Comptime Arguments as Comptime-Symbols with an unknown Comptime value
        Here the order of evaluation in the parameters is given by which parameter references other parameters
    foo :: ($T: B, $B: *T) {}
        The Dependencies of parameters may contain Cycles, which need to be reported as errors
    foo :: ($T: Type) -> T
        With referencing it is also possible that the return-value can reference Comptime arguments
    foo :: ($T: Type, $count: int) -> [count]T
        But the return type cannot create new Polymorphic Parameters, so in this case it can always be analysed last
    foo :: ($T: Type, member_value: #bake(Type)(return smallest_member(T)))
        Bakes can also occur in types, but they must not be executed during analysis.
        Or, to put it more precisely: If a bake contains an unknown Type, it should not be executed
    bar :: (x: foo(int))
        In this case I have a dependency on the header analysis of foo, and I would need to instanciate it
    bar :: ($T: Type, x: foo(T))
        Here I need to instanciate foo during analysis with an Unknown-Type

Implicit Type-Parameter Deduction:
    foo :: (x: T, $T: Type)
    foo(i);
        Here the parameter T is not given by the caller. But T is a type, and it is referenced
        by another parameter, so we can try to deduce the type. So we can analyse the x parameter
        to find that the Type of the parameter is int, and now we can set T to int.
    foo(i, int)
        If a comptime argument is given, it is evaluated first. 
    foo(i, *int)
        Expression-Context can then be used again to do Auto-Address_Of
        

    foo :: (x: Count, $Count: int)
        This already results in an error during Header-Analysis, so this function cannot be called
    foo :: (x: [Count]int, $Count: int)
    foo(int.[1, 2, 3])
        In Theory we could deduce the count, since it is related to the given type of the argument, but
        this should result in an error. This is also the only case where a Comptime-Parameter that
        is not a Type could be deduced, but I this is a small edge case and this should not work.

    foo(i, *int)
        During Instanciation, we first need to check if any of the Polymorphic-Parameters is already given.
        If so, we can set it and analyse the other parameters. If a parameter is not explicitly given, it
        a reference must exist in one of the other parameters or there is an error. If a reference in another
        parameter exists, we need to try to parse the other parameter
    bar :: (a: [Count]T, $Count: int, $T: Type) {}
    bar(int.[1, 2, 3]);
        No parameter is given, so we know we need to deduce Count and Type from Parameter a
        For me this seems like we need to treat Value Parameters different from Type-Parameters.
        Because Type-Inference is definitly something I want, but value inference seems wrong, when
        looking at the following example
    bar :: (a: [Count + 1]int, $Count: int)
    bar(int.[1, 2, 3, 4, 5])
        Here a Human could deduce that Count = 4, but if this cannot work in all cases,
        since expressions can represent arbitrarily complex functions
    bar(.[1, 2, 3], T = int)
        A parameter is given, meaning that we first evaluate the parameter, and afterwards we fit the Count
        Expressions which require Expression-Context (Auto-Array/-Struct/-Enum + casts) 
    bar :: (a: [Count]T, b: T, $Count: int, $T: Type) {}
    bar()

Polymorphic Datatypes:
    Holder :: struct(T: Type) {value: T;}
        Analysing the parameters of a polymorphic struct is the same as those of functions
        Only on structures it is implicit that all parameters are comptime.
    Holder :: struct(T: Type, count: int, default_value: [count]Type) {value: [count]T}
        We can again reference other parameters, so we need a dependency cycle resolve
    Holder :: struct(T: Type, node: Node(T)) {}
        During analysis we may require instanciation of other Polymorphic Objects with unknown types
    x: Holder(String).Value_Type
        Comptime-Parameters of value types should be accessible via member access, and the access is Comptime-Known
    x: Holder;
        The usage of the struct without the comptime parameters results in an error.

Implicit Polymorphic Type-Parameters:
    foo :: (a: $T)
        foo(i)
    foo :: ($T: Type, a: T)
        The upper line does the exact same thing as the lower one, but the function may now be
        called more conveniently
        

    

Polymorphic Datatypes as parameters:
    foo :: (x: Holder(int)) {}
        Poly-Types may be used as parameter types, requiring an analysis
    foo :: ($T: Type, x: Holder(T), y: x.T)
        They may need to be instanciated during analysis with incomplete information
    foo :: (x: Holder)
        Usage of incomplete Types is allowed only in other polymorphic parameters
        This makes the function/datatype that references them also automatically polymorphic
    Hashtable_Info :: struct(x: Hashtable)
        table: *type_of(x)
        smallest_key: x.Key_Type









Comptime Argument: $ before the argument name
    foo :: ($bar: int) {
Implicit Polymorphic Type: $ before a Type-Name
    foo :: (bar: $T)

I think only having Comptime Arguments would not be that hard, although we already get Dependency Problems
    foo :: ($count: int, a: [count]byte) 

Hashset :: struct(Key_Type: Type, hash_fn: (a: Key_Type) -> u64)

Sized_Array :: struct(Value_Type: Type, Size: int, default_value: [Size]Type)
    a: [Size]Value_Type

size_array_init :: (a: $T/Sized_Array, x: [a.Size]a.Value_Type)
    

Instanciation
    foo :: ($count: int)
    foo(15);

Signature-Analysis Order is dependent on 

What happens during Signature Analysis?
    I set a boolean to indicate that polymorpic types are allowed in this context, then I analyse the header

What happens during Header-Analysis?
    First we need to check if the function is polymorphic or if it isn't polymorphic
    I think I want to ignore the type signature during analysis of a 

What happens when Instanciating?

First off all, this requires that the header analysis knows which parameters are polymorphic.

Problem: Header parsing needs to parse the function signature.
Function Signatures alone can also be Expressions.
Can normal function signatures have parameter names/comptime arguments/default arguments?
    bin_op_function: (a: int, b: int) -> int;
    bin_op_function = add_ints;
    bin_op_function = sub_ints;

    bin_op_function: ($count: int, c: int) -> int 
    bin_op_function: (a: int = 5)

So in RC_Code, the Parameter names + information about comptime + default values.
Normally a function signature returns a Type_Type, but when we detect that there
are Polymorphic Arguments involved 

I also think that all functions require parameters names, otherwise it is unspecified wheater
    (int)
is a function taking an integer, or just Type int in parenthesis.
If you want to have a pointer somewhere, and don't write names, just use an alias
    int_fn_type :: (a: int)
    x_fn: int_fn_type = print_double
    bin_op_fn_type :: (a: int, b: int) -> int
    y_fn: bin_op_fn_type = add_int










    



















------------------
----- OTHERS -----
------------------

Metaprogramming:
    In metaprogramming, the Coder creates a piece of code that creates other pieces of Code.
    This is useful in Situations where we otherwise would need to have Code-Duplication,
    or where the generation of the Code could be automated (E.g. Serialization, Printing, Simple Constructor/Destructor logic)

    Compile-Time Code execution is also a means to:
     - Replace traditional Build-Systems
     - Metaprogramming
     - Advanced Error detection/handling
     - Code-Modifications (Automated logging, timing)

Design Discussion: Templates (OLD)
----------------------------
Open Questions:
 * Functions inside templated functions
 * How does type-checking work for templated functions 

Why are templates usefull/what purpose do they have?
They are usefull because there are multiple usecases where a programmer wants to reuse the same Code with
just change some small things about it. For example, to have a typesave Container, withouth templates, 
one would either need other metaprogramming capabilities (E.g. Macros in C, some code-generation tool...)
or sacrifice Type-Safety/Speed (E.g. use Any-Type, Java Object...). 
This is especially true if you want Code that works on multiple types, but requires instances of the type to be on the stack,
since this cannot easily be done without metaprogramming or custom assembly.
Although the main use of this is with containers, it is also usefull for values, e.g. for Vectors, or Conditionall-Compilation, 
or calling different functions without function pointers

Lets first look at how other languages approach this problem:
    Zig: There is the concept of comptime parameters, and types can be returned from functions, which are automatically
    executed at compiletime I guess?
    fn List(comptime T: type) type {
        return struct {
            items: []T,
            len: usize,
        };
    }

    C/C++: Here you use custom template syntax to indicate that the function/struct is using templates
    template<typename T, int count>
    struct Array {
        T data[count];
    };

    Odin: 
    If you put a $ before the variable definition, this means that it must be compile time known:
    foo :: ($T: Type, slice: []T, index: int) -> T {
        return slice[index];
    }
    foo(int, int.[1, 2, 3], 2); // Returns 3

    If you put the $ before a parameter type, this means that the type is infered from the passed parameter
    foo :: (slice: []$T, index: int) -> T {
        return slice[index];
    }
    foo(int.[1, 2, 3], 1); // Returns 2;

    Jai:
    This is basically the same as in Odin, but there are also:
     * #modify
     * #bake (For parameters)

How do I want to approach this problem?
Probably the same as in Jai and Odin, but since I also have comptime code execution it will
be more similar to Jai. 

Two things that bother me:
It seems like both Jai and Odin are kind-of using Duck-Typing in their Generic approach:
    add :: (a: $T, b: T) -> T {return a + b;}
    return_mem_x :: (a: $T) -> int {return a.x;}
I don't understand how this can be analysed to check if the code is correct, e.g.
    foo :: (a: $T)
    {
        x := a.x;
        // Or
        y := bar(a);
        info := type_info(y);
        x = other_fn(x, 15);
        a.x = a.x * x;
        x.value = 15;
    }
To be fair this seems possible to implement, template types are just able to be used in operations, e.g., but the result is not specified

Mem_Interface :: struct(T: Type) {
    access_mem: (value: *T) -> int;
}
But my first thought was that all operations on templated types must also be specified, except assignment
    foo :: (a: $T, $interface: Mem_Interface(T))
    {
        x := interface.access_mem(a);
    }

But do I actually have any good reasons why this shouldn't be allowed?
 * Without this, I can check that the templated thing works without Problems --> Probably wrong because of bake/modifiy
 * With this, I already have easily defined interfaces/traits in the language for metaprogramming

foo :: (a: $T)
{
    #run {
        if (type_info(T).size < 8) {
            Compiler~error("Size of Type must be greater than 8");
        }
    }
    x := false;
    y := x + 5; // Obviously I want this 2 error now
    a :: size_of(T);
    #insert {
        str := "x = x + 1;";
        return str;
    }
    #if a > 16
    {
    }
    #else
    {
    }
}
// To be fair, until I have some crazy stuff, I probably just want to analyse and check if I can detect
// any errors.

partial_order :: (left: $T, right: T) -> bool;
sort_array :: (array: []$T, less_fn: (a: T, b: T) -> bool)
{
    if less_fn(array[j], array[j])
    {
    }
}

The reason that C++ has Contracts, Java has Interfaces + Generict matching thing and Rust has Traits is that
in Polymorphic/Templated/Generic Code it is sometimes requires that a given type has some given properties.
These could be:
    * has a member of name x of type int
    * has a member-function with a given signature
    * has a given unary operator defined on it
    * has an overloaded operator defined

In Jai and Odin, it seems like you just get an error, but I feel like I just want to pass a constant function as parameter to
handle these cases. Maybe I should checkout what Zig does.



All extern source declarations:
#load "whatever.upp";
#extern "cstdio" { ID0 ID1 ID2 }
#extern fn_name :: (); 
#extern lib "test.lib"; 

/*
To be fair at some point I want to be able to load extern sources from code.
#run {
    Compiler~load_project("basic_datastructures.upp");
    Compiler~define_extern_header("adder", (a:int, b:int)->int);
    Compiler~load_c_header("cstdio");
    Compiler~add_lib("test.lib");
}
*/





Design Discussion: Constant/Readonly definitions + Constant Propagation
-----------------------------------------------------------------------
Lexicography:
    Constant ... A value known at compile-time. Can be used for optimizations
    Readonly ... A readonly variable that is only initialized once, at declaration. The value of this variable cannot/should not change

I obviously want to have Constant values, because multiple systems build on it, like templates, bake and compile-time execution.
The syntax is also pretty much set in stone right now, with it being quite similar to the variable definition syntax.

Constant Syntax works well because it supports type, functions and variables (And later maybe modules, but we will see about that):
    x :: 5; // Comptime integer
    x: u64: cast 5; 
    x :: #bake(Node) {...; return list.head;};
    x :: () -> int {};
    int_fn :: () -> int;
    x :: struct {}
    x: struct{a: int};

Why would I want a Readonly System in the language?
 * Parameters should not be changable, to allow the compiler to do optimizations with big structs (e.g. pass as pointer)
 * Comptime values should not be changable, because accessing them will at some point result in an error (Readonly pages in C-compilation)
 * Constant propagation would be a lot easier (Readonly variables can be better propagated then normal variables)
 * Readonly variables may be a cleaner solution in Code sometimes

Design questions:
 * Syntax for readonly
 * Pointer interaction with readonly
 * Struct/Union interaction with readonly
 * Should strings be readonly
 * Casting from/to readonly
 * Transitive readonly?

Thoughts:
    // Readonly values:
    x: readonly float = 3.2; // WORKS
    add :: readonly (a: int, b: int) -> int {return a + b;} // ERROR: expected type, not function
    Node :: readonly struct {value: int};                   // WORKS: Node now cannot be changed...

    // Readonly values cannot be changed
    x = 12.0;                      // ERROR, x is readonly
    node: readonly Node;           // ERROR, node is not initialized
    node: readonly Node = .{...};
    node.next &= whatever_node;    // ERROR, member accesses are not allowed on readonly variables
    xp :&= x;                      // WORKS, xp is now a pointer to readonly int
    xp = 5;                        // ERROR, trying to write to readonly value
    ip: *int = 5;                  
    ip &= x;                       // ERROR, cannot cast from *readonly int to *int

    // Pointers to Readonly values can also not be changed
    x: readonly int = 5;
    xp :&= x;
    xp = 7;                        // ERROR, trying to write to readonly int
    i := 5;                         
    xp = i;                        // WORKS,

    // Because readonly usually requires things to be initialized, it can also be added afterwards
    // This will cause problems with the constant propagation, but should still be possible to implement.
    x: int = 5;
    xp :&= x;
    x = x + 7;
    readonly x;
    xp = 7; // Now the readonly value changed to 7, but it still cannot be changed through the x symbol;

    // Constant structs:

    // Pointers/Slices inside structs:
    Node :: struct {
        value: int;
        next: &Node;
    }
    head: Node;
    head = Node.{value = 0; next &= null;};
    head.value += 1;
    readonly head;

    x := head.value + 1;  // WORKS, reading values is not a problem
    head.value += 1;      // ERROR, Everything that is accessed on a const type is now also const 
    x := head.next.value; // WORKS, 




-------------------------------
--- IMPLEMENTATION THOUGHTS ---
-------------------------------
RUNNABLE/STRUCT_REACHABLE: 
--------------------------
The problem both bake and Compile/Runnable share is that the workload depends
on other Workloads of the same type, e.g.
   * Runnable requires all called functions to be runnable
   * Reachable requires all referenced/contained members to be reachable

This would work fine with a dependency graph, but the problem here is that
 - Circular Dependencies are NOT an Error, but rather something that happens often

This structure seems to imply that these Workloads need to be executed in Clusters, 
where one Cluster is a Set of Workloads that are all reachable from one to another, which is what 
I have done currently. 



CONTROL_FLOW:
-------------
Flow Types:
    * Sequential 
    * Stops      
    * Returns 

Break behavior:
 - If a break is reachable, all blocks up to the broken block have sequential flow

Loop behavior
    Sequential, (Is fine)                               -> Return Sequential
    Stops       (must also be fine because of continue) -> Return Sequential
    Returns     (Error because loop only runs once)     -> Return Returns

Conditional Rules:
 - If all branches return the same flow, the result-flow is that flow
 - If one branch is sequential, the result is sequential
 - Otherwise we have a mix of Stops and Return, which results in Stops

 

DEFERS:
-------
Defer: Mechanics are perfectly fine as they were before.
Triggers:
 - SCOPE/FUNCTION End
 - Return
 - Break/Continue --> Resolves it to the correct outer scope.

Defer stuff to think about
 - No defers inside other defers (I think this should work...)
 - Continue/Break lables must be in the given defer --> I don't think thats true because Defer currently stay in the same scope
 - Returns inside defers are not allowed? (I feel like the usecases of that are very limited)
 - Flow of defer? With no return

So current Solution:
 - No Defers inside Defers
 - No Break/Continue/Return inside defer (Control-Flow Changes)

Implementation:
 - Keep a stack of Modtree_Blocks as active defer blocks
 - Keep a stack of active defers (RC_Statement_Blcoks) 
 - ModTree_Blocks contain the start_defer index 
 - And then I can reevaluate defer on each of those blocks










---------------------------
------ ISSUE TRACKER ------
---------------------------
Long-Term Goals:
----------------
 * Debugger
 * Language features
 * Editor features (Search and replace, jump to definition, show context info, refactoring, Code LOD)
 * Maybe implementing Syntax guided Editor

More Language features to implement:
------------------------------------
 * Tag-System (For constant values, but also other stuff)
   ------- These features are for convenience, but they are not top priority ------
 * Array bounds checking would be good
 * Macro-System
 * Iterators
 * Location-Information (Filename, line_number...)
 * Using statement
 * Context (Used for memory allocations, useful in Multi-Threading)
 * Array size should be u64, then iteration is more tedious (casting), but with iterators it should be fine
 * Literal overhaul (What is a float, what isnt...)
 * Loop statements, maybe loop over array syntax
 * Variation for simple Dynamic dispatch  (Like interfaces in OO..., similar to dynamic in Rust)

Editor features to implement:
-----------------------------
 * Code-Completion using current symbol table
 * Search and Replace
 * CTRL-R for replace word --> I would need Shift-R, since control R is already Redo
 * Visual Mode/Blockmode (Multicursor)/Visual Line
 * Multi-Window support (Multiple Tabs/Vertical or Horizontal splits, Tabs with shift-Tab)
 * Jump to definition with string search (Maybe Ctrl-F)
 * Undo/Redo Tree History
 * Command recording (@ stuff with vim, altough this could maybe be done better)
 * Tab for indentation/Ctrl-Tab for no indentation
 * Formating with = should remove unneccessary whitespaces/add them where needed, and should ignore comments.
    Actually, this should not be hard even without lexer, since we can count the parenthesis, spaces and other stuff...
 * UTF-8 Support?

Editor-Bugs:
------------
 * Alt-Gr spamming still cancels commands (win32 nonsense)
 * Undo-Redo cursor position still sucks a little (Hint: Save positions after/before command)
 * Non ASCII chars destroy some motions (Currently disabled) (Program crashes)

 Vision for the Programming Language:
 ------------------------------------
  * Metaprogramming with Compile-Time code execution
  * Static-Analyser with information exchange
  * Code-Visualizations in Editor (Maybe syntax guided editor)

Ideas that need to be tested:
-----------------------------
 * Implicit parameters in functions, some way of not having to call with all parameters?
 * Using compile-time code-execution for compiler-controll, metaprogramming and static analysis
 * Strong Code-Analysis (Analysing all possible values), with Warning System and user input
 * Tag-System (For const, but also other information, generated by user and compiler)
 * Visualizations for Code, better navigation, module system
 * Non-Destructive Code-Editing for larger modules

 * Tag for not used anymore for variables
 * Compiler supported Dynamic-Array type
 * Hot-Reloading Code
 * Stack-Analysis for Debug information (Or others? E.g. logging)
 * Redirect debug output to application window
 * Error-Handling with the most recent build of the language 

Code Improvements that should be made (When I have time):
----------------------------------------------------
Hashing type signatures for lookup, not array search, or maybe use Graph-like data structure

Differentiation between errors and Warnings

Intermediate Code should keep positional information, so should the bytecode (Will be necessary for the debugger)

At some point the editor should not render at 60 FPS, but rather update when necessary.
Asynchronous Editor-Compiler architecture, lexing/parse/analysing in background when code gets bigger





----------------------------
------ RANTS/THOUGHTS ------
----------------------------

Syntax Design-Principles:
-------------------------
Some characteristics I want:
    - Compression based length (Often used constructs should be short, others can be long)
    - Easy to remember
    - Similar things should look similarly, different things should look different
    - Refactoring should be easy
    - Small refactorings should not cause unpredictable errors

Rant: On Serializability
------------------------
In C and C++, the program does not have access to type-information, meaning that something like automatic serialization is not possible inside the language.
    You could still do this by writing custom tools, which parse the code again (e.g. with clang) to get this type-information
    and create files containing serialization code (Was done by Valve Physics Engine Programmers), but this is -very- tedious and error-prone.
Even if this works, you cannot serialize all data, mainly because of pointers/references, which is the reason you cannot just memcopy data for serialization.
To solve this, some changes to C/C++ are required:
    - Dynamically Allocated arrays must have a size member (e.g. Slices in Upp have a size)
    - Void* may not be used inside these structures (You don't have any type information about that), which is solved with the Any-Type
A problem that still exists is that function pointers will not work/require special serialization effort.

Design Questions:
-----------------
 - Do anonymous structs duck-type?
        No, if this is useful, just define the struct
 - Should the unpacking syntax be allowed for structs as well?
        Yes, why should it not, makes refactoring easier
 - Named argument passing?
        This seems to overlap a lot with struct initilization, since struct initialization IS basically just
        calling a function, but the function exists already for each Struct type
        Maybe I want struct initialization to require naming the arguments, and not for functions
        Not exactly the same, since Union initialization is 
 - With anonymous structs, how are they instanciated?

Design Ideas:
-------------
 - Function parameters are constant, parameters may be passed as pointers if they are too big
 - Functions can only return one value, but it may be an anonymous struct
 - Function return value cannot be dropped, ignore syntax (_ = do_smth();)
 - Unpacking syntax for anonymous structs (x, y = returns_anonymous();)

Design: Multiple returns:
-------------------------
I feel like I want a clear destinction between in, out and in-out parameters of functions
    in: Normal, non-pointer parameters, passed as values
    in-out: Pointer parameters
    out: Return-values

In C you cannot have a clear distinction between these three, because: 
    - You dont want to pass huge structs/unions by pointer  --> Therefore pointer arguments may be in
        In C++ you have constant references, do_smth(const int& smth)
        In Rust/Zig arguments are always constant, and the compiler decides wheter or not they are passed as pointers
    - You cannot have multiple return values                --> Either have a parameter-pointer that is written to, or create and return a struct
        Either support multiple return values, or some tuple types/anonymous structs as returns


Thoughts on pointers:
---------------------
UPDATE:
Pointers acting exactly the same as values is, I think, a great idea. The only problem we have are
situation where a pointer operation could also be used instead of a value operation. 
The question I am having is wheter or not I want to have special syntax to solve this problem. 
The current operations that accept both pointers and values are:
    - Equals-Comparisons (== and !=)
    - Cast to u64 (cast(u64))

Pointers are essential in Computer programming, and I want them in the language
Null seems to be an issue for a lot of programmers, but this should be handled by the static analyzor
Pointers are used for call by reference,
Pointers are also used to have multiple return values
Pointers are also used to not pass huge structures by value, since this requires copying

If you think about function parameters as either in, out or in_out parameters,
pointers in C basically combine all of those into one feature.
This makes some APIs kinda shitty, because seeing a pointer does not provide the user
information about nullable, if its an in, out or in_out value. 

In Upp I think I want a distinction of these type by having the following:
 - Non-Pointer parameters are constant (So that huge structs can be passed internally by pointer without requiring a copy), 
    although I have to think about constant pointers inside of structs
 - Pointer parameters could in theory be all types of parameters (in, out, in_out), but should only be used for in_out
    which is definitly something that is gonna be required quite often
 - With multiple return values there is no need to use pointers for return values

In C pointers are also used as arrays, with pointer arithmetic being a thing (- and + being defined for pointers)
This also leads to the use of stacked pointers, (char** argv, where argv is actually an array to char*, so []String in my language)
The operations required on pointers in C are:
    - Address-Of    int* xp = &x;
    - Dereference   *xp = *xp + 1;
    - Array-Access  item = xp[2];
    - Arithmetic    item = *(xp + 2);
                    ip   = xp + 2;
                    diff = ip - x;

I would like to be pointers to be as interchangable with values as possible, to do that I want
to pass around the expected_type of expressions 




Syntax what operators are in use:
---------------------------------
    +   ... Addition
    -   ... Subtraction, Unary negate
    *   ... Multiplication/Address Of/Pointer type constructor
    &   ... Dereference
    /   ... Division
    %   ... Modulo
    !   ... Logical not

    <   ... Less 
    >   ... Greater
    <=  ... Less Equals
    >=  ... Greater Equals
    ==  ... Equals
    !=  ... Not Equals

    &&  ... Logical And
    ||  ... Logical Or

    ()  ... Function call/Function definition
    []  ... Array definition/Index operation
    {}  ... Code Block operation
    <>  ... Template argument parenthesis

    "   ... String delimiters
    #   ... Extern declarations (#load)
    .   ... Member access/Auto access
    ;   ... Statement delimiter
    ,   ... List item delimiter

    :   ... Variable definition
    :=  ... Define-Infer
    ::  ... Constant Definition
    ->  ... Function return value

    //  ... Single line comment
    /*  ... Multiline comment start
    */  ... Multiline comment end

Currently unused but plans exist:
    _   ... Unused value in multiple returns
    '   ... Char delimiter (Not implemented yet)
    $   ... Fast Template declaration (Not implemented yet)
    @   ... Annotations
    \   ... String delimiters

Open Characters and normal usage
    ~   ... Bitwise Negate
    ^   ... Bitwise XOR
    &   ... Bitwise And
    |   ... Bitwise Or
    >>  ... Bitshift right
    <<  ... Bitshift left
    ?   ... Ternary Operator
       ... Nothing
    




Ideas that just come to mind:
-----------------------------
PrintF Debugging could be replaced with memory visualizer
'Destructors' could be generated with metaprogramming, simple initializors too
Advanced initializers could also be generated with metaprogramming and Annotations
Differentiate States and Tags in the language
Should we have compile-time known arguments, are they the same as template args?
Context struct could live in thread-local memory
Syntax guided editor would be cool I guess
Are stacked pointers a reasonable idea (**int)? Where would i use them
Automated parameter deduction (Like macros taking names from outer scope)
Meta-Programming code modifications may be easy if executed code must be unmodified, and only finaly code is modified
Strings should probably just be byte slices (int + pointer) and be immutable, and we'll just have a String_Builder for output

Code Ideas:
-----------
Type_System should maybe do more, like templates, parameter names, struct names
Symbol template infos add info if symbol is templated
Maybe think about making initializers helpers for expressions/statements?
Const expression evaluation maybe after creation?
Redo template syntax, only allow on functions/structs
Delete IR_Code, just use modtree for translation
Functions may just be compile time known function pointers?
Hardcoded functions should be removed at some point
AST_Node only really needs type and child information for processing, parent and tokenrange could be in another array and multiple structures could exist for this
ModTree and IR_Code Structure is probably not final, but well see when we know more
Workload-Structure may be easier if we could analyse the dependencies beforehand -->
    Maybe implement pre-analysis step, so that Workloads dont need Pauses/Wait for dependencies
Custom allocator for Bytecode-Interpreter, check pointers if they are in range, have custom boolean
    which can be toggled to indicate that C-Functions may be called
Initialize all variables to zero in Bytecode

Thought Run-Down:
-----------------
Maybe remove single statement blocks, e.g. if (bool) return false;
Just replace these with if (cond) {return false;}

Anonymous structs:
    x: struct {name: String; value: int;};
    x.name = "Henlo";
    x.value = 5;
    x = .{name = "Henlo"; value = 15;};

Return value may be struct
    find_elem(...) -> struct{found: bool; index: int;}
    return .{found = true; index = 15;};

Values may be left explicitly uninitialized
    .{found = _, index = 15};

Struct unpacking syntax
    found, index := find_elem(...);
    f2, _ := find_elem(...);

Return values must be explicitly ignored:
    _ = hashtable_add_element(whatever...);

Lambda support
    x := () {print_string("What");};
    adder := (x: int, y: int) -> int {return x + y;};

Choosing enum type
    Something :: enum<int> {}
    Smaller :: enum<byte> {}

Expression_Blocks?
    UPDATE: Currently i think these are a bad idea, because return does something else inside these blocks,
        and I could introduce the concept of yield, which would return a value, but I don't think this is necessarily useful.
        The Idea behind expression blocks would be to give more purpose to an unnamed block. E.g.

        The 2 main use cases I have for these are :
            * a short way to write if's and switches
            * Make sure some values are initialized

            The main use cases I can think of are:
                * a short way to write if's and switches
                * Makes sure values are going to be initialized
                * Gives purpose to values (E.g. you know where the values are going to be used)

            Problems:
                * Syntax overlap inside if, switch, while --> Not true with extra syntax, e.g. #{}
                * They may be quite unnecessary, because function can do the same thing
                * ModTree_Code's structure of expressions and statements isn't exactly build for this
                * Would definitly need some experimentation

        fn_call(15, {if condition {yield make_unknown();} yield make_known(condition);});

    found, index: bool, int = {
        
    }

    of 
    expression.index, expression.is_templated = {

    };

    x = {switch color {case .RED: return 5; case .GREEN: return 2;}}
    x = {if a return 4; return 2;}
    z = 32 + {if alive return 17; return -2;}
    min_member: *Struct_Member = 0;
    {
        curr_min: *Struct_Member = members[0];
        loop mem in members {
            if mem.val < curr_min.value curr_min = mem;
        }
        if (curr_min.alive != 0) {
            min_member = curr_min;
        }
    }
    
    // Why could this be useful? Because it gives the code block a distinct purpose, and its impossible to not set the values 
    found, member: bool, *Member = 
    {
        ret_type :: struct {f: bool; mem: *Member};
        loop a in array {
            if condition_met(a) {
                return ret_type{f = true, mem = a};
            }
        }
        return ret_type{f = false, mem = _};
    } 

    // Also some things can be typed rather quickly
    based_num := {if is_based return 69; return 0;};

NOTES:
------
 * Next stuff:
    - Jump to definition fix --> A separate analysis/code generation phase would be nice
    - Code_Completion
 * Improve Language:
    - Compile time code-evaluation
    - Tag-System + Static Analyis
 * Improve Editor:
    - Multiple projects
    - Code-Completion
    - Search and Replace


Metaprogramming features:
-------------------------
 * Templates (Types for Containers, Functions/Values for algorithms and interfaces)
 * Macro System (Iterators, Scope_Break, Variable_Define, Code_Insertion... lots of convenience functions)
 * Defer
 * #if (Conditional Compilation)
 * Currying
 * AST-Manipulation (Compile time Execution)
 * Generating source-code (Compile time Execution)








 // RENDERING RANT
Ideas for rendering rework:
    - What do we need for rendering:
        * Rendering state (Culling, depth-test, ...)
        * Shader 
        * Mesh (Triangle data, position, normals, uvs, color?)
        * Uniforms (Camera, MVP matrix, shader parameters)
        * Textures
        * Framebuffers (Textures which you can render into)
        * Uniform blocks would be great (Common things like View-projection, time and other parameters)
        * GPU buffers (Where I can easily upload memory for e.g. dynamic mesh data (Text) or instance data)

I think most of the things I already have are pretty nice, but what I also want to do is to the following:
 - Creating/Destroying shaders is kinda annoying
 - Rendering_Core should be a global variable so I don't have to pass it everywhere
 - Maybe I should also take a look at how current calls are made and how I think it would be nicer.
 - For rendering stuff on top of each other, I still think that I want to have something like a RenderPass,
   because otherwise it's going to be hard to have order-independent draw calls 
   (E.g. your code needs to respect the order of the draw-calls you submit)
 - After having implemented a GUI I would like to be able to toggle the different draw calls, and see the effect
    of not doing them immediately on the screen. 
 - Ordering of render-call is also something that I want to keep in mind, e.g. do I want to set the order 
   explicitly via something like adding dependencies on e.g. framebuffer finished or do I want to set this implicitly,
   or do I have to state the order of the rendering calls in code? --> This would be cool if I could set this manually.
   Also I think this has to do with synchronization right? Or does opengl automagically do this for us when rendering into
   a framebuffer and afterwards reading from the same buffer. I guess so.

Use-cases I want to cover:
 - GUI drawing (Mutliple render-calls for each 'plane', e.g. rendering of text and 2D-Shapes in multiple different calls)
   --> Text rendering means rendering textured quads
 - Simple 3D with meshes (E.g. 3D data with normal and others)
 - Custom shaders 
 - Instancing

Question 1: Stateful or not?
    auto& main_pass = rendering_retrieve_renderpass(Type::Backbuffer, ) // Retrieve backbuffer renderpass
    auto& shader = rendering_get_shader("constant_color.glsl")
    renderpass_draw(shader, )
    shader.draw

What is a renderpass:
 * Renderpasses contain/store draw calls
 * A renderpass only has one specific pipeline_state ? and one framebuffer it renders too
 * Renderpasses regulate the order in which draw-calls to opengl are made, and
   all draw calls inside a renderpass may happen at the same time.
 * Renderpasses can contain child passes, a renderpass finishes only if all child-passes are finished
    Is this actually usefull? Maybe for synchronization reasons, but otherwise probably not...

 So i guess I want my text-rendering and my 2D-rendering to be able to generate draw calls for a given renderpass...
 Well obviously if you want to draw text on other text or 2D- things on other 2D things...

 Maybe I want to go the distance and do everything with instancing (Instead of using uniforms)
 This is obviously not always possible, but it would mean that different renderpasses could use the same instance buffer, 
 and everything would be batched automatically. I think this would be fucking sick, so let's do just that.

 So i guess I just want something that does instancing automatically, where I can create draw calls which are potentially in 
 different renderpasses, and if they may be executed at the same time, then this will happen here. Sounds quite nice I guess

 Mesh: Multiple arrays of the same length with different datatypes, like Position3D, uvs, ...
 Mesh_Descriptor: Datatypes contained in the mesh

 Shader Design:
  * Shaders only have per-frame uniforms, and there is a warning if they are not set?

Changes to the current render system:
 * Rendering core will be a global variable (So i don't need to pass shit around) X
 * Rework of Mesh-attributes and GPU-Buffers.
 * Named values for things like framebuffers/shaders/textures/meshes?, so that they are either setup beforehand, or they are looked up per frame.
 * Draw-commands will be added to renderpasses
 * Renderpasses are generated inside the rendering-core

renderer_lookup_renderpass() # Requires that it was already created
renderer_anonymous_renderpass() 
renderer_create_renderpass()
    



Note about the design of this rendering system:
 * It should be flexible and 'nice' to use, so that I can write rendering code faster/cleaner.
    --> Renderpass system should play in here quite nicely. 
 * The design should be done in a way that so that it is possible to optimize the draw calls and stuff at some point.
    --> Note: this means that I don't need to optimize everything right now, but it should be possible to optimize.
 * To generally describe the 2D rendering problem:
    : Every 2D object draws to a rectangular position on the screen. (Or at least we can bound the region, may also be fullscreen)
    : Every 2D object is at a specific depth to one another (Which determines when it is drawn)
    : There are opage objects, where the depth of the objects needs to be respected. (Either via depth testing)
    : There are transparent objects (Note: almost all textures, or for antialiasing), which need to 

So I guess you always want to do 2D rendering in layers, 


So how do I want to deal with dynamic data, e.g. data that you upload to the gpu?
 - In 3D you usually want to have constant meshes, which you don't upload every frame
 - But you would upload some information (e.g. positions and sheet) every frame
 - In 2D I want to upload the whole mesh every frame.

So I want some helper thing that let's me generate the dynamic information every frame.
 --> No, I can generally request an array of dynamic gpu data for a specific mesh, which can then be used in render-calls
 --> Meshes stay completely on the cpu-side too.

So obivously after window creation I have something like this:


Some more notes:
    - Some objects which should not be recreated every frame should have names,
      e.g. framebuffers, shaders --> by filename?, textures --> Size? 
      The names here are then only used for caching the values.

      Shader shader("something something");
      Texture textures...

      // Rendering part
      {
        shader.draw or whatever
      }

      auto framebuffer = make_framebuffer("Icon_Target", 500, 400, RGBA)
      auto pass = make_renderpass(framebuffer)
      auto shader = get_shader("filename");
      pass.draw(shader, core.meshes.quadmesh, {uniform("position", vec4(1, 2, 3, 4))})

      main_pass.depends_on(pass) // So we need to wait until we finished rendering

      shader.set_uniform(uniform("sampler2d", framebuffer.get_color_attachment(0)));



TODO: Rewrite attribute system: You can query the renderer for a set of inputs and then get these.
      Then for mesh creation you 

      I guess VertexDescription consists of a bitmask of multiple attributes then.
        Note: And also in the future of custom attributes (Here the binding location needs to be specified)


      

      The renderpass options need to be set at least once before rendering, and can be updated every frame...
      Mesh somemesh = rendering_core_query_mesh(Vertex_Attribute_Description) --> Creates an empty cpu mesh
      mesh_set_mesh_data(Mesh_Type::Static, Topology::Triangles)
      rendering_core_create_renderpass("main", target = Backbuffer, Clear = True)
      rendering_core_get_shader("whatever")
      rendering_pass_set_uniform() "Sets uniform in the pass for shader"
      rendering_pass_draw() "Also allows to set for uniforms"
      All uniforms need to be set once per frame!

      // Vertex attributes:
      auto pos = rendering_core_query_attribute_vec3("Position3D") --> Returns Attribute<Vec3>*
      auto normal = rendering_core_query_attribute_vec3("Normal3D") --> Returns Attribute<Vec3>*
                        --> query_vertex_attribute("Position3D", Data_Type information) (e.g. vector, matrix, ...)
      auto my_new_mesh = rendering_core_query_mesh("Text_Mesh", Mesh_Type::Static, Topology::Triangles)
      // Fill vertex data
      Array<vec3> positions = mesh_get_attribute_data_array(pos, 128); // Calling this twice within a frame should result in a warning
      positions[0] = ...
      Array<vec3> colors = mesh_get_attribute_data_array(color, 128);
      colors[0] = ...
      // Fill indices
      Array<uint> indices = mesh_get_attribute_data_array(predefined.index, 64);
      indices[0] = ...

      Then the shader generation code can generate the bindings ?
      rendering_core_get_shader("")

// In shader instead of
layout (location = 1) in vec2 position;
#input(Position3D) vec3 position;
// Note that we then still need to check if the variable is actually used or not
New shader syntax:
#VERTEX
All after to next/end is the vertex shader code

uniform vec3 name;
in vec3 pos; //@Position3D --> these lines will be rewritten



#FRAGMENT



Auto-Generating Shaders Ideas:
 - I still want to have hot reloading, so the shaders should be in files
 - I think I want to keep the normal structure of glsl code, so that I can use all things available there.
    * vec4 x
    * x.xxy = vec3(1.0, 0.0, 1.0)
 - I don't want to write a custom parser/analyer yet, I just want a quick and dirty way of doing this
 - This probably also means that syntax/error highlighting will probably not work in Visual studio editor,
    or maybe it will if we do some special things (Like maybe including all used values in a single file)

Design/general Ideas:
 - Shader code is in general quite functional, since you cannot really modify global state (e.g. runs on gpu)
 - Reduce duplicate code for each new shader 
    (e.g. passing 2d positions for 2d objects through vertex shader, multiplying position by mvp matrix)
    Shading: calculating normalized eye to view direction and other stuff
 - Vertex shader needs to set
 - Uniforms and shader inputs are still normally accessible through the shader code and our other systems,
    this is solely about creating the shader code and reducing manual work
 - I want some of the common variables (pos3D) to have names, and pieces of shader code can depend on
    previously defined names and generate values for new variables, or update current variable values
 - So shader snippet defines inputs and outputs
 - What about snippets that define multiple outputs, and some outputs aren't used?
    * Should we rely on the compiler to optimize these away?  --> Yes
    * We could also make snippets more complicated by allowing outputs to depend on inputs
    * Lets go with the simple solution at first
    * I guess if it comes to performance its always possible to take multiple small snippets and put them
        in a bigger snippet, which reduces the flexibility, but increases other stuff...
 - I guess some variables could also be directly connected to Vertex-Attributes? ... Not sure if this is a good idea
    Otherwise we will need a vertex_attribute loader for each vertex attribute. --> Is probably the better idea
 - How should inputs and outputs between vertex and fragmentshader happen?
    * Maybe if we know what types the different shaders are, different shader stages can just refer to previously generated values.
      I think this is the smartest move, the only thing you have to worry about is maybe interpolation 
      (e.g. some option if you want to interpolate flat or non-perspective corrected)
 - Is there any way this can be made somewhat typesafe, or do I have to rely on dynamic checking... I guess I have to
 - Maybe we also want to have general access to functions with this, since this really isn't a feature in glsl
    E.g. you probably would need to start including headers if you do function things, but then you cannot access input/outputs
 - Also accessing our custom ubos is a given, I think
 - Problem: If I write the composition of the snippets in c++ (E.g. which interact to form a shader)
            this composition cannot be hot-reloaded... But this would probably be one of the interesting parts of this whole project
            This could be solved if we had 'another' syntax for composing shaders...
            Here the problem is that if we want our composition to be dependent on some program logic 
            (Maybe performance options or generating different shaders based on what which textures are available)
            this could not be expressed in the other syntax...
            To be fair this isn't the main problem now, and I also want to be able to write normal shaders without any other influences...
            But I guess both solutions could co-exist, so I probably want to start out with the C++ controled version

Example:
    C++ code:
        shader_generator_make({
            // Vertex part
            snippet_from_attribute(predefined.position2D, "pos"),
            snippet("uvs_from_pos"), --> Requires pos
            // Either
                snippet_custom("gl_Position = vec4(pos, 0.0, 0.0);", Shader_Type::Vertex)
                snippet_vertex_position("pos"),  // Somehow we need to define the vertex position in the end...

            // Fragment part:
            frag_output("o_color");
            snippet("")
            snippet("background")
        });

Some things where duplication happens:
 - Solid color vs shaded vs textured...
 
   






Side project:
    Since i don't have too much time currently to work on the compiler, I would like to start
    working on smaller things that aren't directly connected to the compiler, like the GUI or formatting or stuff like that.
    Some things that come to mind:
        - Text-Rendering update: Maybe rewrite the text renderer
            I just think that doing the distance field calculation with the sdf that I currently have would be a lot nicer
            than the things that I am currently doing. 
            Also maybe you know not make it suck ass, lol
        - Custom allocators:
            * In C++ this would be interesting to explore if a global variable would actually be more convenient to use.
              I guess I also would tmp_allocator_allocate_array()
                                   allocator_allocate_array()
              The problem here is how Datastructures would handle allocators, e.g.
              We would probably need to rewrite our datastructures to use allocator functions.

              Also should datastructures keep a pointer to the allocator they were created with?
              This would probably be necessary, but it seems a little annoying when thinking about strings and simple arrays
              where the size of the allocator pointer would almost double the whole thing.
               
              Then this should also work with threads and fibers, so fiber-handles would also need to store the current
              allocator if we want to keep the logic consisten.
        - Immediate mode GUI and 2D-drawing:
        - Rework of rendering logic with rendering passes.
        - More vim commands for editor, e.g. movement and stuff.
        - Dynamic code formating in the editor

I decided to work on the rendering side again. 

